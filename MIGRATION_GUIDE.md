# FactWave Backend Migration Guide

## ğŸ“‹ Overview
ì´ ë¬¸ì„œëŠ” FactWave ë°±ì—”ë“œë¥¼ Upstage Solar-pro2ì—ì„œ OpenAI GPT-4o-minië¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ê³ , CrewAIì˜ Structured Outputì„ ì ìš©í•˜ë©°, ëª¨ë“  í”„ë¡¬í”„íŠ¸ë¥¼ YAMLì—ì„œ ì¤‘ì•™ ê´€ë¦¬í•˜ë„ë¡ ë³€ê²½í•œ ê³¼ì •ì„ ìƒì„¸íˆ ê¸°ë¡í•©ë‹ˆë‹¤.

## ğŸ¯ Migration Goals
1. **LLM Provider ë³€ê²½**: Upstage Solar-pro2 â†’ OpenAI GPT-4o-mini
2. **Structured Output ì ìš©**: JSON íŒŒì‹± ì˜¤ë¥˜ ê°ì†Œë¥¼ ìœ„í•œ Pydantic ëª¨ë¸ ê¸°ë°˜ êµ¬ì¡°í™”
3. **í”„ë¡¬í”„íŠ¸ ì¤‘ì•™í™”**: ëª¨ë“  ì—ì´ì „íŠ¸ ì„¤ì •ì„ YAML íŒŒì¼ì—ì„œ ê´€ë¦¬
4. **Confidence í•„ë“œ ì œê±°**: LLM ì¼ê´€ì„± ë¬¸ì œ í•´ê²°

## ğŸ“ ë³€ê²½ëœ íŒŒì¼ êµ¬ì¡°

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agents/           # ì—ì´ì „íŠ¸ ì •ì˜ (YAMLì—ì„œ ì„¤ì • ë¡œë“œ)
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ prompts.yaml  # ëª¨ë“  í”„ë¡¬í”„íŠ¸ì™€ ì—ì´ì „íŠ¸ ì„¤ì • ì¤‘ì•™í™”
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ crew.py       # Structured Output ì ìš©
â”‚   â”‚   â””â”€â”€ streaming_crew.py  # Confidence ì œê±°
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ responses.py  # Pydantic ì‘ë‹µ ëª¨ë¸
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ llm_config.py # CrewAI LLM ì„¤ì •
â”‚       â””â”€â”€ prompt_loader.py  # YAML í”„ë¡¬í”„íŠ¸ ë¡œë”
```

## ğŸ”„ Migration Steps

### Step 1: LLM Provider ë³€ê²½

#### 1.1 ì˜ì¡´ì„± ì—…ë°ì´íŠ¸
```python
# ì´ì „ (ì˜ëª»ëœ ì ‘ê·¼)
from langchain_openai import ChatOpenAI

# ì´í›„ (ì˜¬ë°”ë¥¸ CrewAI ë°©ì‹)
from crewai import LLM
```

#### 1.2 LLM ì„¤ì • íŒŒì¼ ìƒì„± (`app/utils/llm_config.py`)
```python
from crewai import LLM
from typing import Type, Optional
from pydantic import BaseModel

class StructuredLLM:
    """CrewAI LLM with structured output support"""
    
    @staticmethod
    def create_structured_llm(
        response_model: Optional[Type[BaseModel]] = None,
        temperature: float = 0,
        max_tokens: Optional[int] = None
    ) -> LLM:
        base_config = {
            "model": "gpt-4o-mini",  # OpenAI ëª¨ë¸
            "temperature": temperature,
            "max_tokens": max_tokens or 3000,
        }
        
        # Pydantic ëª¨ë¸ì„ ì§ì ‘ response_formatì— ì „ë‹¬
        if response_model:
            base_config["response_format"] = response_model
        
        return LLM(**base_config)
```

#### 1.3 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
`.env` íŒŒì¼ì— OpenAI API í‚¤ ì¶”ê°€:
```
OPENAI_API_KEY=sk-proj-xxxxx
```

### Step 2: Structured Output êµ¬í˜„

#### 2.1 Pydantic ì‘ë‹µ ëª¨ë¸ ì •ì˜ (`app/models/responses.py`)
```python
from typing import List, Literal
from pydantic import BaseModel, Field

class Step1Analysis(BaseModel):
    """Step 1: ì´ˆê¸° ë¶„ì„ ì‘ë‹µ êµ¬ì¡°"""
    agent_name: str = Field(description="ì—ì´ì „íŠ¸ ì´ë¦„")
    verdict: Literal[
        "ì°¸", "ëŒ€ì²´ë¡œ_ì°¸", "ë¶€ë¶„ì _ì°¸", "ë¶ˆí™•ì‹¤", "ì •ë³´ë¶€ì¡±", 
        "ë…¼ë€ì¤‘", "ë¶€ë¶„ì _ê±°ì§“", "ëŒ€ì²´ë¡œ_ê±°ì§“", "ê±°ì§“", "ê³¼ì¥ë¨", 
        "ì˜¤í•´ì†Œì§€", "ì‹œëŒ€ì°©ì˜¤"
    ] = Field(description="íŒì • ê²°ê³¼")
    key_findings: List[str] = Field(description="í•µì‹¬ ë°œê²¬ì‚¬í•­")
    evidence_sources: List[str] = Field(description="ê·¼ê±° ì¶œì²˜")
    reasoning: str = Field(description="íŒì • ê·¼ê±°")

class Step2Debate(BaseModel):
    """Step 2: í† ë¡  ì‘ë‹µ êµ¬ì¡°"""
    agent_name: str = Field(description="ì—ì´ì „íŠ¸ ì´ë¦„")
    agreements: List[str] = Field(description="ë™ì˜í•˜ëŠ” ì ")
    disagreements: List[str] = Field(description="ì´ê²¬ì´ë‚˜ ë³´ì™„ì ")
    additional_perspective: str = Field(description="ì¶”ê°€ ê´€ì ")
    final_verdict: Literal[...] = Field(description="ìµœì¢… íŒì •")

class Step3Synthesis(BaseModel):
    """Step 3: ìµœì¢… ì¢…í•© ì‘ë‹µ êµ¬ì¡°"""
    final_verdict: Literal[...] = Field(description="ìµœì¢… íŒì •")
    key_agreements: List[str] = Field(description="ì£¼ìš” í•©ì˜ì ")
    key_disagreements: List[str] = Field(description="ì£¼ìš” ë¶ˆì¼ì¹˜ì ")
    verdict_reasoning: str = Field(description="ìµœì¢… íŒì • ê·¼ê±°")
    summary: str = Field(description="ì¢…í•© ìš”ì•½")
```

#### 2.2 Taskì— Structured Output ì ìš© (`app/core/crew.py`)
```python
from ..models.responses import Step1Analysis, Step2Debate, Step3Synthesis

def create_step1_tasks(self, statement: str) -> List[Task]:
    tasks = []
    for agent_name, agent_instance in self.agents.items():
        if agent_name != "super":
            task = Task(
                description=description,
                agent=agent_instance.get_agent("step1"),
                expected_output="íŒì •, ê·¼ê±°, ì‹ ë¢°ë„ë¥¼ í¬í•¨í•œ êµ¬ì¡°í™”ëœ ë¶„ì„",
                callback=task_callback_func,
                output_json=Step1Analysis  # âœ… Structured Output ì ìš©
            )
            tasks.append(task)
    return tasks
```

### Step 3: í”„ë¡¬í”„íŠ¸ ì¤‘ì•™í™”

#### 3.1 YAML ì„¤ì • íŒŒì¼ ìƒì„± (`app/config/prompts.yaml`)
```yaml
# ì—ì´ì „íŠ¸ ê¸°ë³¸ ì„¤ì •
agents:
  academic:
    role: "í•™ìˆ  ì—°êµ¬ ì „ë¬¸ê°€"
    goal: "í•™ìˆ  ë…¼ë¬¸ê³¼ ì—°êµ¬ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì£¼ì¥ì„ ê²€ì¦"
    backstory: |
      í•™ìˆ  ì—°êµ¬ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 20ë…„ ê²½ë ¥ì˜ ë°•ì‚¬ë¡œì„œ...
      
      ë„êµ¬ í™œìš©:
      â€¢ Wikipedia: ê¸°ë³¸ ê°œë… í™•ì¸
      â€¢ OpenAlex: í•™ìˆ ë…¼ë¬¸ ê²€ìƒ‰ (ì£¼ë ¥)
      â€¢ ArXiv: ìµœì‹  ì—°êµ¬ ë™í–¥
      
      ì‘ë‹µ ì›ì¹™:
      â€¢ ë©”íƒ€ë¶„ì„ê³¼ ë¦¬ë·° ë…¼ë¬¸ ìš°ì„ 
      â€¢ ìµœì†Œ 3-5ê°œ ë…¼ë¬¸ ê²€í† 
      â€¢ í•™ê³„ ì»¨ì„¼ì„œìŠ¤ ì¤‘ì‹¬
  
  news:
    role: "ë‰´ìŠ¤ ê²€ì¦ ì „ë¬¸ê°€"
    goal: "ì–¸ë¡  ë³´ë„ì™€ ë¯¸ë””ì–´ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì£¼ì¥ì„ ê²€ì¦"
    backstory: |
      ë‰´ìŠ¤ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤...

# Stepë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
step1:
  general:
    template: |
      ì£¼ì¥: {statement}
      
      {role}ë¡œì„œ ì´ ì£¼ì¥ì„ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
      
      ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
      {{
        "agent_name": "{agent_name}",
        "verdict": "[ì„ íƒì§€ ì¤‘ í•˜ë‚˜]",
        "key_findings": ["í•µì‹¬ ë°œê²¬ì‚¬í•­ë“¤"],
        "evidence_sources": ["ì¶œì²˜ë“¤"],
        "reasoning": "íŒì • ê·¼ê±°"
      }}

step2:
  template: |
    ì£¼ì¥: {statement}
    
    ë‹¤ë¥¸ ì „ë¬¸ê°€ë“¤ì˜ ë¶„ì„ì„ ê²€í† í•˜ê³  í† ë¡ í•˜ì„¸ìš”...

step3:
  template: |
    ëª¨ë“  ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… íŒì •ì„ ë‚´ë¦¬ì„¸ìš”...
```

#### 3.2 PromptLoader í´ë˜ìŠ¤ êµ¬í˜„ (`app/utils/prompt_loader.py`)
```python
import yaml
from pathlib import Path
from typing import Dict, Any

class PromptLoader:
    """YAML íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ê´€ë¦¬"""
    
    def __init__(self, config_path: str = None):
        if config_path is None:
            config_path = Path(__file__).parent.parent / "config" / "prompts.yaml"
        self.config_path = Path(config_path)
        self.prompts = self._load_prompts()
    
    def get_agent_config(self, agent_name: str) -> Dict[str, str]:
        """íŠ¹ì • ì—ì´ì „íŠ¸ì˜ ì„¤ì • ë°˜í™˜"""
        agents = self.prompts.get('agents', {})
        if agent_name not in agents:
            raise KeyError(f"Agent '{agent_name}' not found")
        return agents[agent_name]
    
    def get_step1_prompt(self, agent_type: str, statement: str, 
                        role: str = None, agent_name: str = None) -> str:
        """Step 1 í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        template = self.prompts['step1'][agent_type]['template']
        return template.format(
            statement=statement, 
            role=role, 
            agent_name=agent_name
        )
```

#### 3.3 ì—ì´ì „íŠ¸ íŒŒì¼ ìˆ˜ì • (ì˜ˆ: `app/agents/academic_agent.py`)
```python
from .base import FactWaveAgent
from ..utils.prompt_loader import PromptLoader
from ..services.tools import WikipediaSearchTool, OpenAlexTool, ArxivSearchTool

class AcademicAgent(FactWaveAgent):
    """í•™ìˆ ì  ì§€ì‹ê³¼ ê³¼í•™ì  ì¶”ë¡ ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì¥ì„ ë¶„ì„í•˜ëŠ” ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        # YAMLì—ì„œ ì„¤ì • ë¡œë“œ
        prompt_loader = PromptLoader()
        agent_config = prompt_loader.get_agent_config('academic')
        
        super().__init__(
            role=agent_config['role'],
            goal=agent_config['goal'],
            backstory=agent_config['backstory']
        )
        
        # ë„êµ¬ ì´ˆê¸°í™”
        self.tools = [
            WikipediaSearchTool(),
            OpenAlexTool(),
            ArxivSearchTool()
        ]
```

### Step 4: Confidence í•„ë“œ ì œê±°

#### 4.1 Streaming Crew ìˆ˜ì • (`app/core/streaming_crew.py`)
```python
# ì´ì „ (confidence í¬í•¨)
await self.ws_manager.emit({
    "type": "task_completed",
    "content": {
        "analysis": analysis,
        "verdict": verdict,
        "confidence": confidence,  # âŒ ì œê±°
    }
})

# ì´í›„ (confidence ì œê±°)
await self.ws_manager.emit({
    "type": "task_completed",
    "content": {
        "analysis": analysis,
        "verdict": verdict,  # âœ… verdictë§Œ ì „ì†¡
    }
})
```

#### 4.2 ê´€ë ¨ ë©”ì„œë“œ ì œê±°
- `_extract_confidence()` ë©”ì„œë“œ ì œê±°
- `_calculate_confidence_from_verdict()` ë©”ì„œë“œ ì œê±°
- `_calculate_weighted_confidence()` ë©”ì„œë“œ ì œê±°

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (`test_structured_output.py`)
```python
from app.core.crew import FactWaveCrew
from app.models.responses import Step1Analysis

def test_structured_output():
    test_statement = "í•œêµ­ì˜ ìµœì €ì„ê¸ˆì€ 2024ë…„ ê¸°ì¤€ ì‹œê°„ë‹¹ 9,860ì›ì´ë‹¤"
    crew = FactWaveCrew()
    tasks = crew.create_step1_tasks(test_statement)
    
    for task in tasks:
        # output_jsonì´ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert hasattr(task, 'output_json')
        assert task.output_json == Step1Analysis
        print(f"âœ… Task configured with structured output")

if __name__ == "__main__":
    test_structured_output()
```

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. ë°±ì—”ë“œ ì„œë²„ ì‹¤í–‰
```bash
cd backend
uv run python -m app.api.server
```

### 2. WebSocket í…ŒìŠ¤íŠ¸
```bash
cd backend
uv run python test_websocket_client.py "ê²€ì¦í•  ë¬¸ì¥"
```

### 3. í”„ë¡ íŠ¸ì—”ë“œ ì‹¤í–‰
```bash
cd frontend_wonjun
npm run dev
```

## ğŸ“Š ê°œì„  íš¨ê³¼

### Before
- **LLM**: Upstage Solar-pro2 (í•œêµ­ì–´ íŠ¹í™”)
- **ì‘ë‹µ í˜•ì‹**: ë¹„êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸
- **í”„ë¡¬í”„íŠ¸ ê´€ë¦¬**: ê° ì—ì´ì „íŠ¸ íŒŒì¼ì— í•˜ë“œì½”ë”©
- **JSON íŒŒì‹±**: ì •ê·œì‹ê³¼ ë¬¸ìì—´ íŒŒì‹± ì˜ì¡´
- **Confidence**: LLMì´ ìƒì„±í•˜ëŠ” ë¶ˆì•ˆì •í•œ ê°’

### After
- **LLM**: OpenAI GPT-4o-mini (ë²”ìš©, ë¹„ìš© íš¨ìœ¨ì )
- **ì‘ë‹µ í˜•ì‹**: Pydantic ëª¨ë¸ ê¸°ë°˜ êµ¬ì¡°í™”
- **í”„ë¡¬í”„íŠ¸ ê´€ë¦¬**: YAML íŒŒì¼ì—ì„œ ì¤‘ì•™ ê´€ë¦¬
- **JSON íŒŒì‹±**: CrewAIê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬
- **Confidence**: ì œê±° (verdictë§Œìœ¼ë¡œ ì¶©ë¶„)

## ğŸ” í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### 1. CrewAIì˜ ì˜¬ë°”ë¥¸ ì‚¬ìš©ë²•
- `langchain_openai.ChatOpenAI` âŒ
- `crewai.LLM` âœ…
- Pydantic ëª¨ë¸ì„ ì§ì ‘ `response_format`ì— ì „ë‹¬

### 2. Structured Outputì˜ ì¥ì 
- JSON íŒŒì‹± ì˜¤ë¥˜ ëŒ€í­ ê°ì†Œ
- í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì•ˆì •ì ì¸ ë°ì´í„° ì²˜ë¦¬
- íƒ€ì… ì•ˆì •ì„±ê³¼ ìë™ ê²€ì¦

### 3. í”„ë¡¬í”„íŠ¸ ì¤‘ì•™í™”ì˜ ì´ì 
- ì—ì´ì „íŠ¸ ì„¤ì • ë³€ê²½ ì‹œ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”
- í”„ë¡¬í”„íŠ¸ A/B í…ŒìŠ¤íŠ¸ ìš©ì´
- ë‹¤êµ­ì–´ ì§€ì› í™•ì¥ ê°€ëŠ¥

## ğŸ“ ì¶”ê°€ ê³ ë ¤ì‚¬í•­

### í–¥í›„ ê°œì„  ê°€ëŠ¥ ì˜ì—­
1. **Multi-model ì§€ì›**: ë‹¤ì–‘í•œ LLM Provider ì§€ì›
2. **í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬**: Gitìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ë³€ê²½ ì´ë ¥ ì¶”ì 
3. **ë™ì  í”„ë¡¬í”„íŠ¸ ë¡œë”©**: ì„œë²„ ì¬ì‹œì‘ ì—†ì´ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸
4. **ì‘ë‹µ ìºì‹±**: ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•œ ìºì‹œ êµ¬í˜„

### ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸
- API ë¹„ìš© ì¶”ì  (OpenAI Dashboard)
- ì‘ë‹µ ì‹œê°„ ì¸¡ì •
- Structured Output ì„±ê³µë¥ 
- ì—ëŸ¬ ë¡œê·¸ ë¶„ì„

## ğŸ“š ì°¸ê³  ìë£Œ
- [CrewAI Documentation](https://docs.crewai.com/)
- [OpenAI API Reference](https://platform.openai.com/docs/)
- [Pydantic Documentation](https://docs.pydantic.dev/)
- [CrewAI Structured Output Guide](https://docs.crewai.com/concepts/structured-outputs)

---

*Last Updated: 2025-08-18*
*Author: Claude Code Assistant*